{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c0fc97-03a1-4567-afc2-bc1c009f58ee",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:36px; font-weight:bold; color:#4A4A4A; background-color:#fff6e4; padding:10px; border:3px solid #f5ecda; border-radius:6px\">\n",
    "    Medical Cost Prediction\n",
    "    <p style=\"text-align:center; font-size:14px; font-weight:normal; color:#4A4A4A; margin-top:12px;\">\n",
    "        Author: Jens Bender <br> \n",
    "        Created: December 2025<br>\n",
    "        Last updated: February 2026\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb46ade-5325-4a6f-a876-a89fa84ff886",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Imports</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fb71d4-23c3-4427-83da-a9f1b304504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick  # to format axis ticks\n",
    "import seaborn as sns\n",
    "\n",
    "# Data preprocessing (scikit-learn)\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, \n",
    "    OneHotEncoder, \n",
    "    OrdinalEncoder\n",
    ")\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform  # for random hyperparameter values\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor \n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, \n",
    "    mean_absolute_percentage_error, \n",
    "    r2_score\n",
    ")\n",
    "\n",
    "# Local imports\n",
    "from src.pipeline import create_preprocessing_pipeline\n",
    "\n",
    "# Configuration\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ff7f1-cfe6-4266-9598-f8fa77825c29",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Data Loading and Inspection</h1>\n",
    "</div>\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Load the MEPS-HC 2023 data from the <code>h251.sas7bdat</code> file (SAS V9 format) into a Pandas DataFrame.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6566250d-933c-4ee9-950f-80f916b616b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load data using 'latin1' encoding because MEPS SAS files don't store text as UTF-8 and instead use Western European (ISO-8859-1), also known as latin1.\n",
    "    df = pd.read_sas(\"../data/h251.sas7bdat\", format=\"sas7bdat\", encoding=\"latin1\")\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please check the file path.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"Error: The file is empty.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"Error: The file content could not be parsed.\")\n",
    "except PermissionError:\n",
    "    print(\"Error: Permission denied when accessing the file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bd5af1-3289-4fd7-8ccd-f9d9b49bc884",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> üìå Initial data inspection to understand the structure of the dataset and detect obvious issues.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723eebff-80f4-4e7e-9e94-6af8816a1870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DataFrame info to check the number of rows and columns, data types and missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140a985d-3c7e-492f-9f34-a17df4f2c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top five rows of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a06f785-b1fe-45ee-bfb2-224cb1235bf1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Data Preprocessing</h1>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è <strong>Note:</strong> Kept column names in ALL CAPS to ensure consistency with official <b><a href=\"../docs/references/h251doc.pdf\">MEPS documentation</a></b>, <b><a href=\"../docs/references/h251cb.pdf\">codebook</a></b>, and <b><a href=\"../docs/references/data_dictionary.md\">data dictionary</a></b>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f5bc54-6ee9-4419-8811-86799d1874f0",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Handling Duplicates</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Identify duplicates based on:\n",
    "<ul>\n",
    "    <li><strong>All columns</strong>: To detect exactly identical rows.</li>\n",
    "    <li><strong>ID column only</strong>: To ensure that no two people share the same ID.</li>\n",
    "    <li><strong>All columns except ID</strong>: To catch \"hidden\" duplicates where the same respondent may have been recorded twice under different IDs.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e6bf46-8cf6-4991-bea4-c2b167ff91c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicates based on all columns\n",
    "df.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843f382f-e7d9-46c3-b43a-5e6b5f8eb85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicates based on the ID column\n",
    "df.duplicated([\"DUPERSID\"]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07847cb-f83b-405f-b585-24f953c1cb6b",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ‚úÖ No duplicates were found based on all columns or the ID column.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e58b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicates based on all columns except ID columns\n",
    "id_columns = [\"DUPERSID\", \"DUID\", \"PID\", \"PANEL\"]\n",
    "duplicates_without_id = df.duplicated(subset=df.columns.drop(id_columns), keep=False)\n",
    "duplicates_without_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360131b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show duplicates\n",
    "df[duplicates_without_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a4d05-0800-486e-9802-7b438182e952",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f7fff8; padding:15px; border:3px solid #e0f0e0; border-radius:6px;\">\n",
    "    üí° <b>Insight:</b> Detected 3 pairs (6 rows) of  duplicates based on all columns except ID columns. \n",
    "    <p style=\"margin-top:10px; margin-bottom:0px\">\n",
    "        These respondents have identical values across all 1,300+ columns except for their IDs. They appear to be young siblings (ages 1 and 5) from the same household with identical parent-reported health data, sample weights, and costs. Analysis suggests they are valid respondents rather than \"ghost\" records. Regardless, they will be excluded when filtering for the adult target population.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19272d24-c56d-4a19-9ffc-dfd82d02fff1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Variable Selection</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Filter the following 29 columns (out of 1,374):\n",
    "    <ul style=\"margin-bottom:0px\">\n",
    "        <li><b>ID:</b> Unique identifier for each respondent (<code>DUPERSID</code>).</li>\n",
    "        <li><b>Sample Weights:</b> Ensures population representativeness (<code>PERWT23F</code>).</li>\n",
    "        <li><b>Candidate Features:</b> 26 variables selected for their consumer accessibility, beginning-of-year measurement, and predictive power.</li> \n",
    "        <li><b>Target Variable:</b> Total out-of-pocket health care costs (<code>TOTSLF23</code>).</li>\n",
    "    </ul>\n",
    "    <br>\n",
    "    <b>Rationale:</b> For a detailed breakdown of the target variable selection and feature selection criteria, see the <b><a href=\"../docs/specs/technical_specifications.md\">Technical Specifications</a></b> and <b><a href=\"../docs/research/candidate_features.md\">Candidate Features Research</a></b>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aee5394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to keep \n",
    "columns_to_keep = [\n",
    "    # 1. ID\n",
    "    \"DUPERSID\",\n",
    "    \n",
    "    # 2. Sample Weights\n",
    "    \"PERWT23F\", \n",
    "\n",
    "    # 3 Candidate Features.\n",
    "    # 3.1 Demographics\n",
    "    \"AGE23X\", \"SEX\", \"REGION23\", \"MARRY31X\",\n",
    "    \n",
    "    # 3.2 Socioeconomic\n",
    "    \"POVCAT23\", \"FAMSZE23\", \"HIDEG\", \"EMPST31\",\n",
    "    \n",
    "    # 3.3 Insurance & Access\n",
    "    \"INSCOV23\", \"HAVEUS42\",\n",
    "    \n",
    "    # 3.4 Perceived Health & Lifestyle\n",
    "    \"RTHLTH31\", \"MNHLTH31\", \"ADSMOK42\",\n",
    "    \n",
    "    # 3.5 Limitations & Symptoms\n",
    "    \"ADLHLP31\", \"IADLHP31\", \"WLKLIM31\", \"COGLIM31\", \"JTPAIN31_M18\",\n",
    "    \n",
    "    # 3.6 Chronic Conditions\n",
    "    \"HIBPDX\", \"CHOLDX\", \"DIABDX_M18\", \"CHDDX\", \"STRKDX\", \"CANCERDX\", \"ARTHDX\", \"ASTHDX\", \n",
    "    \n",
    "    # 4. Healthcare Expenditure (Target)\n",
    "    \"TOTSLF23\"\n",
    "]\n",
    "\n",
    "# Drop all other columns (keeping 29 out of 1,374)\n",
    "df = df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c814318-58f4-4411-b9c9-08b40dfdb7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display labels of features and target for plotting\n",
    "display_labels = {\n",
    "    # Target\n",
    "    \"TOTSLF23\": \"Out-of-Pocket Costs\",\n",
    "    \n",
    "    # Demographics\n",
    "    \"AGE23X\": \"Age\",\n",
    "    \"SEX\": \"Sex\",\n",
    "    \"REGION23\": \"Region\",\n",
    "    \"MARRY31X\": \"Marital Status\",\n",
    "    \"FAMSZE23\": \"Family Size\",\n",
    "    \n",
    "    # Socioeconomic\n",
    "    \"POVCAT23\": \"Poverty Category\",\n",
    "    \"HIDEG\": \"Education\",\n",
    "    \"EMPST31\": \"Employment\",\n",
    "    \n",
    "    # Insurance & Access\n",
    "    \"INSCOV23\": \"Insurance\",\n",
    "    \"HAVEUS42\": \"Usual Source of Care\",\n",
    "    \n",
    "    # Perceived Health & Lifestyle\n",
    "    \"RTHLTH31\": \"Physical Health\",\n",
    "    \"MNHLTH31\": \"Mental Health\",\n",
    "    \"ADSMOK42\": \"Smoker\",\n",
    "    \n",
    "    # Limitations\n",
    "    \"ADLHLP31\": \"ADL Help\",\n",
    "    \"IADLHP31\": \"IADL Help\",\n",
    "    \"WLKLIM31\": \"Walking Limitation\",\n",
    "    \"COGLIM31\": \"Cognitive Limitation\",\n",
    "    \"JTPAIN31_M18\": \"Joint Pain\",\n",
    "    \n",
    "    # Chronic Conditions\n",
    "    \"HIBPDX\": \"High Blood Pressure\",\n",
    "    \"CHOLDX\": \"High Cholesterol\",\n",
    "    \"DIABDX_M18\": \"Diabetes\",\n",
    "    \"CHDDX\": \"Coronary Heart Disease\",\n",
    "    \"STRKDX\": \"Stroke\",\n",
    "    \"CANCERDX\": \"Cancer\",\n",
    "    \"ARTHDX\": \"Arthritis\",\n",
    "    \"ASTHDX\": \"Asthma\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab17d4-5459-46b5-b864-3e992c45e4c8",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Target Population Filtering</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Filter rows to match the target population (target audience for app) based on the following criteria:\n",
    "    <ul style=\"margin-bottom:0px\">\n",
    "        <li><b>Positive person weight (<code>PERWT23F > 0</code>):</b> Drop respondents with a person weight of zero (i.e., 456 respondents). These individuals are considered \"out-of-scope\" for the full-year population (e.g., they joined the military, were institutionalized, or moved abroad).</li>\n",
    "        <li><b>Adults (<code>AGE23X >= 18</code>):</b> Drop respondents under age 18 (i.e., 3796 respondents), as the medical cost planner app targets adults.</li>\n",
    "    </ul>\n",
    "    <br>\n",
    "    <b>Note:</b> Keeps 14,768 out of 18,919 respondents.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80beb46-f9cf-4fc3-a80e-dee328f07f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter DataFrame \n",
    "df = df[(df[\"PERWT23F\"] > 0) & (df[\"AGE23X\"] >= 18)].copy() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b1bd90-9e04-432f-bb46-ef10b0530f31",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Handling Data Types</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Identify and convert incorrect storage data types.\n",
    "    <ul>\n",
    "        <li><b>ID:</b> <code>DUPERSID</code> is an identifier, not a quantity. Converting them to <code>string</code> prevents unintended math.</li>\n",
    "        <li><b>Sample Weights:</b> <code>PERWT23F</code> contains decimal precision critical for population-level estimates. Must remain <code>float</code>.</li>\n",
    "        <li><b>Candidate Features:</b> The SAS loader stored all 26 features as floats by default. Although many features are categorical and represent integer codes (e.g., 1=Male, 2=Female), they are maintained as <code>float</code> for three practical reasons:\n",
    "            <ul>\n",
    "                <li>Missing Value Compatibility: In standard Pandas, <code>np.nan</code> is a floating-point object. Assigning it to an integer column automatically casts back to <code>float64</code>.</li>\n",
    "                <li>Data Preprocessing Consistency: scikit-learn transformers (e.g., <code>SimpleImputer</code>, <code>StandardScaler</code>) internally use floats and automatically convert numerical inputs to <code>float</code>, even when using <code>set_config(transform_output=\"pandas\")</code>. Keeping them as floats avoids redundant type casting.</li>\n",
    "                <li>Model Consistency: Most machine learning models (e.g., XGBoost, Linear Regression) internally use floats and automatically convert numerical inputs to <code>float</code> during training and inference. Keeping them as floats avoids redundant type casting.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><b>Target Variable:</b> <code>TOTSLF23</code> was stored as <code>float</code> by the SAS loader. Although it is rounded to whole dollars in the MEPS data, it is kept as <code>float</code> for data preprocessing and model consistency and to avoid redundant type casting, as ML models deliver <code>float</code> predictions during training and inference.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b59357-2082-44a7-9862-589ccd104e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify storage data types (defaulted to float/object by SAS loader)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca8040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ID to string\n",
    "df[\"DUPERSID\"] = df[\"DUPERSID\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e853c-104e-4b59-8be1-443e08d263f6",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> \n",
    "    üìå Define semantic data types of features (numerical, binary, nominal, ordinal) for downstream tasks like EDA, further preprocessing steps, and machine learning.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a53022c-1a35-4087-a122-950bead7dc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define semantic data types\n",
    "numerical_features = [\"AGE23X\", \"FAMSZE23\", \"RTHLTH31\", \"MNHLTH31\"]\n",
    "binary_features = [\n",
    "    \"SEX\", \"HAVEUS42\", \"ADSMOK42\", \"ADLHLP31\", \"IADLHP31\", \n",
    "    \"WLKLIM31\", \"COGLIM31\", \"JTPAIN31_M18\", \"HIBPDX\", \"CHOLDX\", \n",
    "    \"DIABDX_M18\", \"CHDDX\", \"STRKDX\", \"CANCERDX\", \"ARTHDX\", \"ASTHDX\"\n",
    "]\n",
    "nominal_features = [\"REGION23\", \"MARRY31X\", \"EMPST31\", \"INSCOV23\"]\n",
    "ordinal_features = [\"POVCAT23\", \"HIDEG\"]\n",
    "\n",
    "# Combined feature sets\n",
    "categorical_features = nominal_features + ordinal_features + binary_features\n",
    "all_features = numerical_features + categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c1c55-f1a7-4322-8f2b-d0e4475d767d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Standardizing Missing Values</h1>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è <b>Pandas Missing Value Types</b>:\n",
    "    <ul style=\"margin-bottom:0px\">\n",
    "        <li><b>np.nan:</b> Standard missing value indicator (technically a float); often the default in Pandas for numerical data.</li>\n",
    "        <li><b>pd.NA:</b> Unified missing value indicator for modern nullable data types (mostly integer and boolean).</li>\n",
    "        <li><b>None:</b> Python's native type; often used for object and string data.</li>\n",
    "        <li><b>pd.NaT:</b> For datetime and timedelta data types.</li>\n",
    "    </ul>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è <b>MEPS Missing Value Codes:</b>\n",
    "    <ul style=\"margin-bottom:0px\">\n",
    "        <li><b>-1 INAPPLICABLE:</b> Variable does not apply (structural skip).</li>\n",
    "        <li><b>-7 REFUSED:</b> Person refused to answer.</li>\n",
    "        <li><b>-8 DON'T KNOW:</b> Person did not know the answer.</li>\n",
    "        <li><b>-9 NOT ASCERTAINED:</b> Administrative or technical error in collection.</li>\n",
    "        <li><b>-15 CANNOT BE COMPUTED:</b> Incomplete data for a constructed variable.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf771bde-99a8-4618-8c2d-82de58a9a3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Pandas missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4fd4f1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå <b>Handle MEPS-Specific Missing Values and Skip Patterns</b>\n",
    "    <br><br>\n",
    "    <b>Understanding Skip Patterns</b><br>\n",
    "    Skip patterns (routing logic) are used in surveys to ensure respondents only answer questions relevant to them, reducing burden and improving data quality. For analysis, this creates \"structural\" missingness (coded as -1 Inapplicable) can often be recovered by looking at the respondent's path through the survey.\n",
    "    <br><br>\n",
    "    <b>Recovering Implied Values:</b>  \n",
    "    <ul>\n",
    "        <li><b>Smoker (<code>ADSMOK42</code>):</b> This question is only asked if the respondent already confirmed smoking 100+ cigarettes in their life. Those who said \"No\" skip this and are coded -1. In this project, these \"Never Smokers\" are mapped to \"No\" (2).</li>\n",
    "        <li><b>Joint Pain (<code>JTPAIN31_M18</code>):</b> Respondents who already reported an arthritis diagnosis (<code>ARTHDX = 1</code>) earlier in the interview skip this question and are coded -1. Since arthritis inherently involves joint symptoms, these values are mapped to \"Yes\" (1).</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae9015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover implied values\n",
    "# Smoker: Convert -1 (Never Smoker) to 2 (No)\n",
    "df.loc[df[\"ADSMOK42\"] == -1, \"ADSMOK42\"] = 2\n",
    "\n",
    "# Joint Pain: Convert -1 to 1 (Yes) only if they have Arthritis \n",
    "df.loc[(df[\"JTPAIN31_M18\"] == -1) & (df[\"ARTHDX\"] == 1), \"JTPAIN31_M18\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a45ca1e-7a62-4ee5-9815-47a528cae451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify MEPS missing values \n",
    "missing_codes = [-1, -7, -8, -9, -15]\n",
    "missing_frequency_df = pd.DataFrame({code: (df == code).sum() for code in missing_codes})\n",
    "missing_frequency_df[\"TOTAL\"] = missing_frequency_df.sum(axis=1)\n",
    "missing_frequency_df[\"PERCENTAGE\"] = (missing_frequency_df[\"TOTAL\"] / len(df) * 100).round(2)\n",
    "missing_frequency_df.sort_values(\"TOTAL\", ascending=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82708ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all MEPS missing codes to np.nan\n",
    "df = df.replace(missing_codes, np.nan)\n",
    "\n",
    "# Verify results\n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2413d70-ad80-4f97-af40-0f1e56a10b78",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Train-Validation-Test Split</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\">\n",
    "    üìå Split the data into 80% for training, 10% for validation, and 10% for testing.\n",
    "    <br><br>\n",
    "    <b>Why not a simple random split?</b><br>\n",
    "    Healthcare costs exhibit a zero-inflated, heavily right-skewed distribution (see <a href=\"#target-variable\"><b>Target Variable EDA</b></a>). The primary risk of a random split in healthcare cost data is the \"luck-of-the-draw\" misdistribution of super-spenders. Since the extreme tail of the distribution (the top 1%) accounts for a massive share of total population spending, omitting even a few of these individuals from the test set‚Äîor over-representing them in the train set‚Äîcreates catastrophic \"metric drift.\" This makes performance indicators (like R¬≤ or MSE) highly volatile and unreliable for predicting real-world financial risk. \n",
    "    <br><br>\n",
    "    <b>Why not a quartile or quintile split?</b><br>\n",
    "    Standard quartile or quintile bins are too coarse to capture the extreme tail. Because healthcare costs are so concentrated, a quintile-based split (top 20%) would treat a respondent at the 81st percentile (e.g., \\$2,000) the same as a \"super-spender\" at the 99.9th percentile (e.g., \\$100,000). Only high-resolution, non-linear strata at the 95th and 99th percentiles can guarantee that these \"Black Swan\" cases are balanced across all subsets.\n",
    "    <br><br>\n",
    "    <b>Distribution-Informed Stratified Split</b><br>\n",
    "    To ensure the model is evaluated on a representative mirror of the population, I use a distribution-informed stratified split:\n",
    "    <ul>\n",
    "        <li><b>Mitigating 'Black Swan' Risks (Primary):</b> Uses high-resolution non-linear bins (80, 95, and 99th percentiles) to force the inclusion of extreme high-cost individuals in all sets, preventing unstable performance metric fluctuations.</li>\n",
    "        <li><b>Preserving the Zero-Hurdle (Secondary):</b> Guarantees that the 22.3% of zero-cost individuals remain identical across all sets, ensuring consistent evaluation of the model's ability to predict cost occurrence.</li>\n",
    "        <li><b>Capturing the Pareto Distribution:</b> Prevents evaluation bias by ensuring the 20% of spenders who drive ~80% of the total economic burden are proportionally represented in the test set.</li>\n",
    "    </ul>\n",
    "    <b>Strata Distribution</b>\n",
    "    <table style=\"margin-left:0; margin-top:20px; margin-bottom:20px\">\n",
    "        <tr>\n",
    "            <th style=\"background-color:#f5ecda;\">Bin</th>\n",
    "            <th style=\"background-color:#f5ecda;\">Category</th>\n",
    "            <th style=\"background-color:#f5ecda;\">Percentile (of Positives)</th>\n",
    "            <th style=\"background-color:#f5ecda;\">Train (80%)</th>\n",
    "            <th style=\"background-color:#f5ecda;\">Val (10%)</th>\n",
    "            <th style=\"background-color:#f5ecda;\">Test (10%)</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"background-color:#fff6e4; text-align:center;\"><b>0</b></td>\n",
    "            <td style=\"background-color:#fff6e4;\">Zero Costs</td>\n",
    "            <td style=\"background-color:#fff6e4;\">N/A (Hurdle)</td>\n",
    "            <td style=\"background-color:#fff6e4; text-align:center;\">2,640</td>\n",
    "            <td style=\"background-color:#fff6e4; text-align:center;\">330</td>\n",
    "            <td style=\"background-color:#fff6e4; text-align:center;\">330</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"background-color:#f5ecda; text-align:center;\"><b>1</b></td>\n",
    "            <td style=\"background-color:#f5ecda;\">Low Spend</td>\n",
    "            <td style=\"background-color:#f5ecda;\">0 - 50%</td>\n",
    "            <td style=\"background-color:#f5ecda; text-align:center;\">4,587</td>\n",
    "            <td style=\"background-color:#f5ecda; text-align:center;\">573</td>\n",
    "            <td style=\"background-color:#f5ecda; text-align:center;\">574</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"background-color:#fff6e4; text-align:center;\"><b>2</b></td>\n",
    "            <td style=\"background-color:#fff6e4;\">Moderate</td>\n",
    "            <td style=\"background-color:#fff6e4;\">50 - 80%</td>\n",
    "            <td style=\"background-color:#fff6e4; text-align:center;\">2,752</td>\n",
    "            <td style=\"background-color:#fff6e4; text-align:center;\">344</td>\n",
    "            <td style=\"background-color:#fff6e4; text-align:center;\">344</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"background-color:#f5ecda; text-align:center;\"><b>3</b></td>\n",
    "            <td style=\"background-color:#f5ecda;\">High Spend</td>\n",
    "            <td style=\"background-color:#f5ecda;\">80 - 95%</td>\n",
    "            <td style=\"background-color:#f5ecda; text-align:center;\">1,376</td>\n",
    "            <td style=\"background-color:#f5ecda; text-align:center;\">172</td>\n",
    "            <td style=\"background-color:#f5ecda; text-align:center;\">172</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"background-color:#fff6e4; text-align:center;\"><b>4</b></td>\n",
    "            <td style=\"background-color:#fff6e4;\">Very High</td>\n",
    "            <td style=\"background-color:#fff6e4;\">95 - 99%</td>\n",
    "            <td style=\"background-color:#fff6e4; text-align:center;\">367</td>\n",
    "            <td style=\"background-color:#fff6e4; text-align:center;\">46</td>\n",
    "            <td style=\"background-color:#fff6e4; text-align:center;\">46</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"background-color:#f5ecda; text-align:center;\"><b>5</b></td>\n",
    "            <td style=\"background-color:#f5ecda;\">Massively High</td>\n",
    "            <td style=\"background-color:#f5ecda;\">99 - 99.9%</td>\n",
    "            <td style=\"background-color:#f5ecda; text-align:center;\">83</td>\n",
    "            <td style=\"background-color:#f5ecda; text-align:center;\">10</td>\n",
    "            <td style=\"background-color:#f5ecda; text-align:center;\">10</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"background-color:#fff6e4; text-align:center;\"><b>6</b></td>\n",
    "            <td style=\"background-color:#fff6e4;\">Super Spenders</td>\n",
    "            <td style=\"background-color:#fff6e4;\">Top 0.1%</td>\n",
    "            <td style=\"background-color:#fff6e4; text-align:center;\">12</td>\n",
    "            <td style=\"background-color:#fff6e4; text-align:center;\">1</td>\n",
    "            <td style=\"background-color:#fff6e4; text-align:center;\">2</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b09251-1df0-4598-9da1-70ae4d6307bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X features and y target\n",
    "X = df.drop(\"TOTSLF23\", axis=1)\n",
    "y = df[\"TOTSLF23\"]\n",
    "\n",
    "# Helper function for distribution-informed stratification\n",
    "def create_stratification_bins(y):\n",
    "    # Initialize strata series \n",
    "    strata = pd.Series(index=y.index, dtype=int)\n",
    "    \n",
    "    # Bin 0: Zero Costs (Handle the hurdle separately)\n",
    "    is_zero = (y == 0)\n",
    "    strata[is_zero] = 0\n",
    "    \n",
    "    # Custom non-linear quantiles for positive values to capture the tail\n",
    "    positive_y = y[~is_zero]\n",
    "    bins = [0, 0.5, 0.8, 0.95, 0.99, 0.999, 1.0]\n",
    "    \n",
    "    # Assign positive spenders to bins 1 through 5 \n",
    "    # Note: labels=False returns the bin indices (0-4) instead of Interval objects (e.g., [0, 150.5]). We add 1 to shift the indices to 1-5 with 0 being reserved for the zero cost bin.\n",
    "    # Note: duplicates=\"drop\" avoids errors if multiple quantiles (e.g., 0.95 and 0.99) result in the same cost value.\n",
    "    strata[~is_zero] = pd.qcut(positive_y, q=bins, labels=False, duplicates=\"drop\") + 1\n",
    "    return strata\n",
    "\n",
    "# Generate the stratification column \n",
    "y_strata = create_stratification_bins(y)\n",
    "\n",
    "# Perform the first distribution-informed stratified split (80% Train, 20% Temp)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=RANDOM_STATE, stratify=y_strata\n",
    ")\n",
    "\n",
    "# Re-calculate strata for the temporary set to ensure the 50/50 split is also representative\n",
    "temp_strata = create_stratification_bins(y_temp)\n",
    "\n",
    "# Perform the second stratified split (resulting in 10% Val, 10% Test)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=RANDOM_STATE, stratify=temp_strata\n",
    ")\n",
    "\n",
    "# Helper function to verify the stratification splits\n",
    "def verify_split(y_subset, y_subset_strata, name):\n",
    "    # Calculate relative frequencies of the bins\n",
    "    strata_freq = y_subset_strata.value_counts(normalize=True).sort_index() * 100\n",
    "    \n",
    "    # Calculate key distribution metrics\n",
    "    stats = {\n",
    "        \"Samples\": len(y_subset),\n",
    "        \"Mean Cost\": y_subset.mean(),\n",
    "        \"Median Cost\": y_subset.median(),\n",
    "        \"Max Cost\": y_subset.max()\n",
    "    }\n",
    "    \n",
    "    # Merge metrics and strata proportions \n",
    "    for i, freq in strata_freq.items():\n",
    "        stats[f\"Bin {int(i)}\"] = freq\n",
    "        \n",
    "    return pd.Series(stats, name=name)\n",
    "\n",
    "# Create DataFrame of split verification statistics\n",
    "split_verification_df = pd.concat([\n",
    "    verify_split(y, y_strata, \"Total Dataset\"),\n",
    "    verify_split(y_train, y_strata.loc[y_train.index], \"Train (80%)\"),\n",
    "    verify_split(y_val, y_strata.loc[y_val.index], \"Validation (10%)\"),\n",
    "    verify_split(y_test, y_strata.loc[y_test.index], \"Test (10%)\")\n",
    "], axis=1).T\n",
    "\n",
    "# Delete temporary variables to free up memory\n",
    "del X_temp, y_temp, temp_strata, y_strata\n",
    "\n",
    "# Display the verification DataFrame (format for readability) \n",
    "split_verification_df.style.format(\"{:,.1f}\") \\\n",
    "            .format(\"{:,.0f}\", subset=[\"Samples\", \"Max Cost\"]) \\\n",
    "            .format(\"${:,.0f}\", subset=[\"Mean Cost\", \"Median Cost\", \"Max Cost\"]) \\\n",
    "            .format(\"{:.2f}%\", subset=[\"Bin 0\", \"Bin 1\", \"Bin 2\", \"Bin 3\", \"Bin 4\", \"Bin 5\", \"Bin 6\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f939345",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f7fff8; padding:15px; border:3px solid #e0f0e0; border-radius:6px;\">\n",
    "    üí° <b>Insight:</b> The distribution-informed stratification successfully created representative and robust data splits.\n",
    "    <ul style=\"margin-top:10px; margin-bottom:0px\">\n",
    "        <li><b>Structural Precision:</b> Relative frequencies of all strata (Bins 0‚Äì6) are near-perfectly preserved across all splits, ensuring that the zero-inflation and the Pareto-style concentration are balanced.</li>\n",
    "        <li><b>Mitigation of \"Metric Drift\":</b> By forcing the inclusion of extreme high-cost individuals (99.9th percentile) in the Test set, we prevent \"luck-of-the-draw\" bias and ensure that performance metrics (e.g., R¬≤) are robust against catastrophic outliers.</li>\n",
    "        <li><b>Representative Benchmarking:</b> The stability of central tendencies (Median Cost) confirms that the typical patient profile is identical in each set, allowing for reliable and generalizable model evaluation.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a3aa41-defd-4d13-ad64-01f9392107b1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Handling Missing Values</h2>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Identification</strong> <br>\n",
    "    üìå Identify missing values.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8bac2f-548a-440c-a28e-bd4b0db28a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table for missing values\n",
    "missing_value_df = pd.DataFrame({\n",
    "    \"Training\": X_train.isnull().sum(),\n",
    "    \"Validation\": X_val.isnull().sum(),\n",
    "    \"Test\": X_test.isnull().sum(),\n",
    "})\n",
    "# Add target variable missings\n",
    "missing_value_df.loc[\"TOTSLF23\"] = [\n",
    "    y_train.isnull().sum(),\n",
    "    y_val.isnull().sum(),\n",
    "    y_test.isnull().sum(),\n",
    "]\n",
    "# Display table (sorted and with percentages)\n",
    "missing_value_df.sort_values(\"Training\", ascending=False).style.format({\n",
    "    \"Training\": lambda x: f\"{x} ({x / len(X_train) * 100:.1f}%)\",\n",
    "    \"Validation\": lambda x: f\"{x} ({x / len(X_val) * 100:.1f}%)\",\n",
    "    \"Test\": lambda x: f\"{x} ({x / len(X_test) * 100:.1f}%)\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ac506e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "<div style=\"background-color:#f7fff8; padding:15px; border:3px solid #e0f0e0; border-radius:6px;\">\n",
    "    üí° <b>Insight:</b> Missing value analysis reveals a high level of data integrity and consistency across all splits.\n",
    "    <ul style=\"margin-top:10px; margin-bottom:0px\">\n",
    "        <li><b>High Data Quality:</b> Missingness is quite low (Maximum ~3.8% for Usual Source of Care), with most features well below 1%, minimizing the risk of imputation bias.</li>\n",
    "        <li><b>Consistent Splits:</b> Missing value frequencies are near-identical across Training, Validation, and Test sets, suggesting the stratification did not introduce feature bias.</li>\n",
    "        <li><b>Key Variable Completeness:</b> Expected cost drivers such as Age, Sex, Region, Poverty Status, Insurance, and the Target Variable are 100% complete.</li>\n",
    "        <li><b>Implication for App Design:</b> The 100% completeness of demographics justifies making them required in the app, while high completeness allows us to safely treat unchecked boxes as an explicit \"No\" (0) rather than a missing value.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc0a75-87e7-40ae-92f0-909c23a14fbd",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Imputation</strong> <br>\n",
    "    üìå Impute missing values. Use the median for numerical features and the mode (most frequent value) for categorical features.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574ad01a-7a60-4588-93d8-eb44e31cd15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate median for each numerical feature from training data\n",
    "medians = X_train[numerical_features].median()\n",
    "\n",
    "# Impute median in training, validation, and test data\n",
    "X_train[numerical_features] = X_train[numerical_features].fillna(medians)\n",
    "X_val[numerical_features] = X_val[numerical_features].fillna(medians)\n",
    "X_test[numerical_features] = X_test[numerical_features].fillna(medians)\n",
    "\n",
    "# Verify results\n",
    "pd.DataFrame({\n",
    "    \"Training\": X_train[numerical_features].isnull().sum(),\n",
    "    \"Validation\": X_val[numerical_features].isnull().sum(),\n",
    "    \"Test\": X_test[numerical_features].isnull().sum(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2337e97e-a17c-45ea-a064-35aa25228ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mode for each categorical feature from training data\n",
    "modes = X_train[categorical_features].mode().loc[0]\n",
    "\n",
    "# Impute mode in training, validation, and test data\n",
    "X_train[categorical_features] = X_train[categorical_features].fillna(modes)\n",
    "X_val[categorical_features] = X_val[categorical_features].fillna(modes)\n",
    "X_test[categorical_features] = X_test[categorical_features].fillna(modes)\n",
    "\n",
    "# Verify results\n",
    "pd.DataFrame({\n",
    "    \"Training\": X_train[categorical_features].isnull().sum(),\n",
    "    \"Validation\": X_val[categorical_features].isnull().sum(),\n",
    "    \"Test\": X_test[categorical_features].isnull().sum(),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c24d8f7-0321-48d7-9d8d-33ce8a9c3d74",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Handling Outliers</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0781e82-0843-4202-b789-97614b30db64",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">3SD Method</h3>\n",
    "</div>\n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> üìå Identify and remove univariate outliers in numerical columns by applying the 3 standard deviation rule (3SD). A data point is considered an outlier if it falls more than 3 standard deviations above or below the mean of the column.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf92003-4fdc-4bf4-a1f7-ec794d0e75d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom scikit-learn transformer class to identify and remove outliers using the 3SD method\n",
    "class OutlierRemover3SD(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df, numerical_columns):\n",
    "        # Convert single column string to list\n",
    "        if isinstance(numerical_columns, str):\n",
    "            self.numerical_columns_ = [numerical_columns]\n",
    "        else:\n",
    "            self.numerical_columns_ = numerical_columns\n",
    "            \n",
    "        # Calculate statistics (mean, std, cutoff values) for each column\n",
    "        self.stats_ = pd.DataFrame(index=self.numerical_columns_)\n",
    "        self.stats_[\"mean\"] = df[self.numerical_columns_].mean()\n",
    "        self.stats_[\"std\"] = df[self.numerical_columns_].std()\n",
    "        self.stats_[\"lower_cutoff\"] = self.stats_[\"mean\"] - 3 * self.stats_[\"std\"]\n",
    "        self.stats_[\"upper_cutoff\"] = self.stats_[\"mean\"] + 3 * self.stats_[\"std\"]\n",
    "        \n",
    "        # Create masks for filtering outliers \n",
    "        self.masks_ = (df[self.numerical_columns_] >= self.stats_[\"lower_cutoff\"]) & (df[self.numerical_columns_] <= self.stats_[\"upper_cutoff\"])  # masks by column\n",
    "        self.final_mask_ = self.masks_.all(axis=1)  # single mask across all columns\n",
    "     \n",
    "        # Calculate number of outliers\n",
    "        self.stats_[\"n_outliers\"] = (~self.masks_).sum()  # by column\n",
    "        self.stats_[\"pct_outliers\"] = self.stats_[\"n_outliers\"] / len(df) * 100  # by column\n",
    "        self.outliers_ = (~self.final_mask_).sum()  # across all columns\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Create masks for df (can be a different df than during fit; e.g. X_train, X_test)\n",
    "        self.masks_ = (df[self.numerical_columns_] >= self.stats_[\"lower_cutoff\"]) & (df[self.numerical_columns_] <= self.stats_[\"upper_cutoff\"])  # masks by column\n",
    "        self.final_mask_ = self.masks_.all(axis=1)  # single mask across all columns\n",
    "        \n",
    "        # Remove outliers based on the final mask\n",
    "        return df[self.final_mask_]\n",
    "\n",
    "    def fit_transform(self, df, numerical_columns):\n",
    "        # Perform both fit and transform \n",
    "        return self.fit(df, numerical_columns).transform(df)\n",
    "\n",
    "\n",
    "# Initialize outlier remover \n",
    "outlier_remover_3sd = OutlierRemover3SD()\n",
    "\n",
    "# Fit outlier remover to training data\n",
    "outlier_remover_3sd.fit(X_train, numerical_features)\n",
    "\n",
    "# Show outliers in training data\n",
    "print(f\"Training Data: Identified {outlier_remover_3sd.outliers_} rows ({outlier_remover_3sd.outliers_ / len(outlier_remover_3sd.final_mask_) * 100:.1f}%) with outliers.\\n\")\n",
    "print(\"Outlier statistics by column:\")\n",
    "outlier_remover_3sd.stats_.style.format({\n",
    "    \"mean\": \"{:.2f}\",\n",
    "    \"std\": \"{:.2f}\",\n",
    "    \"lower_cutoff\": \"{:.2f}\",\n",
    "    \"upper_cutoff\": \"{:.2f}\",\n",
    "    \"n_outliers\": \"{:.0f}\",\n",
    "    \"pct_outliers\": \"{:.1f}%\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d294087-69ad-42a9-a858-24d3508050e5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">1.5 IQR Method </h3>\n",
    "</div> \n",
    "\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> üìå Identify and remove univariate outliers in numerical columns using the 1.5 interquartile range (IQR) rule. A data point is considered an outlier if it falls more than 1.5 interquartile ranges above the third quartile (Q3) or below the first quartile (Q1) of the column.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2aa140-47de-46aa-adf7-0fafe1ce16cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom scikit-learn transformer class to identify and remove outliers using the 1.5 IQR method\n",
    "class OutlierRemoverIQR(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df, numerical_columns):\n",
    "        # Convert single column string to list\n",
    "        if isinstance(numerical_columns, str):\n",
    "            self.numerical_columns_ = [numerical_columns]\n",
    "        else:\n",
    "            self.numerical_columns_ = numerical_columns\n",
    "        \n",
    "        # Calculate statistics (quartiles, interquartile range, cutoff values) for each column\n",
    "        self.stats_ = pd.DataFrame(index=self.numerical_columns_)\n",
    "        self.stats_[\"Q1\"] = df[self.numerical_columns_].quantile(0.25)\n",
    "        self.stats_[\"Q3\"] = df[self.numerical_columns_].quantile(0.75)\n",
    "        self.stats_[\"IQR\"] = self.stats_[\"Q3\"] - self.stats_[\"Q1\"]\n",
    "        self.stats_[\"lower_cutoff\"] = self.stats_[\"Q1\"] - 1.5 * self.stats_[\"IQR\"]\n",
    "        self.stats_[\"upper_cutoff\"] = self.stats_[\"Q3\"] + 1.5 * self.stats_[\"IQR\"]\n",
    "\n",
    "        # Create masks for filtering outliers \n",
    "        self.masks_ = (df[self.numerical_columns_] >= self.stats_[\"lower_cutoff\"]) & (df[self.numerical_columns_] <= self.stats_[\"upper_cutoff\"])  # masks by column\n",
    "        self.final_mask_ = self.masks_.all(axis=1)  # single mask across all columns\n",
    "\n",
    "        # Calculate number of outliers\n",
    "        self.stats_[\"n_outliers\"] = (~self.masks_).sum()  # by column\n",
    "        self.stats_[\"pct_outliers\"] = self.stats_[\"n_outliers\"] / len(df) * 100  # by column\n",
    "        self.outliers_ = (~self.final_mask_).sum()  # across all columns\n",
    "               \n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Create masks for df (can be a different df than during fit; e.g. X_train, X_test)\n",
    "        self.masks_ = (df[self.numerical_columns_] >= self.stats_[\"lower_cutoff\"]) & (df[self.numerical_columns_] <= self.stats_[\"upper_cutoff\"])  # masks by column\n",
    "        self.final_mask_ = self.masks_.all(axis=1)  # single mask across all columns\n",
    "        \n",
    "        # Remove outliers based on the final mask\n",
    "        return df[self.final_mask_]\n",
    "\n",
    "    def fit_transform(self, df, numerical_columns):\n",
    "        # Perform both fit and transform\n",
    "        return self.fit(df, numerical_columns).transform(df)\n",
    "\n",
    "\n",
    "# Initialize outlier remover \n",
    "outlier_remover_iqr = OutlierRemoverIQR()\n",
    "\n",
    "# Fit outlier remover to training data\n",
    "outlier_remover_iqr.fit(X_train, numerical_features)\n",
    "\n",
    "# Show outliers by column for training data\n",
    "print(f\"Training Data: Identified {outlier_remover_iqr.outliers_} rows ({outlier_remover_iqr.outliers_ / len(outlier_remover_iqr.final_mask_) * 100:.1f}%) with outliers.\\n\")\n",
    "print(\"Outliers statistics by column:\")\n",
    "outlier_remover_iqr.stats_.style.format({\n",
    "    \"Q1\": \"{:.1f}\",\n",
    "    \"Q3\": \"{:.1f}\",\n",
    "    \"IQR\": \"{:.1f}\",\n",
    "    \"lower_cutoff\": \"{:.1f}\",\n",
    "    \"upper_cutoff\": \"{:.1f}\",\n",
    "    \"n_outliers\": \"{:.0f}\",\n",
    "    \"pct_outliers\": \"{:.1f}%\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eab7459-5de3-4365-8cb0-74feee8e4887",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Summary</h2>\n",
    "</div>\n",
    "\n",
    "- **Data Loading**: Imported MEPS-HC 2023 SAS data using `pandas` `read_sas`.\n",
    "- **Handling Duplicates**: Verified the absence of duplicates based on the ID column, complete rows, and all columns except ID.\n",
    "- **Variable Selection**: Filtered 29 essential columns (target variable, candidate features, ID, sample weights) from the original 1,374 columns.\n",
    "- **Target Population Filtering**: Filtered rows for adults with positive person weights (14,768 out of 18,919 respondents).\n",
    "- **Handling Data Types**: Converted ID to string and maintained features and target as floats to ensure compatibility with scikit-learn transformers and models. Defined semantic data types for all features (numerical, binary, nominal, ordinal).\n",
    "- **Standardizing Missing Values**: Recovered values from survey skip patterns and converted MEPS-specific missing codes to `np.nan`.\n",
    "- **Train-Validation-Test Split**: Split data into training (80%), validation (10%), and test (10%) sets using a distribution-informed stratified split to balance zero-inflation and extreme tail of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c355c2-6e31-44b4-942a-9809fa990ded",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#2c699d; color:white; padding:15px; border-radius:6px;\">\n",
    "    <h1 style=\"margin:0px\">Exploratory Data Analysis (EDA)</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ede20-f3f6-4774-8c25-54b6feb8333e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#3d7ab3; color:white; padding:12px; border-radius:6px;\">\n",
    "    <h2 style=\"margin:0px\">Univariate EDA</h2>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Analyze the distribution of a single column using descriptive statistics and visualizations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e08b38-c905-4f2f-9cc1-b9eddac2a5da",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Sample Weights</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    üìå Examine descriptive statistics and visualize the distribution of the sample weights. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6a8c48-ef92-43c9-945a-0c6fb4317910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics of sample weights\n",
    "df[\"PERWT23F\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607693e7-1135-458c-b3ba-71f1713bf7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of sample weights\n",
    "df[\"PERWT23F\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67c166f-877a-46d2-b039-4381c4ba4f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of sample weights\n",
    "sns.histplot(df[\"PERWT23F\"])\n",
    "plt.gca().xaxis.set_major_formatter(mtick.StrMethodFormatter(\"{x:,.0f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba6e20d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f7fff8; padding:15px; border:3px solid #e0f0e0; border-radius:6px;\">\n",
    "    üí° <b>Insight:</b> Using sample weights in machine learning models is essential to correct for oversampling and ensure that predictions are representative of the U.S. civilian noninstitutionalized adult population. \n",
    "    <ul style=\"margin-top:10px; margin-bottom:0px\">\n",
    "        <li><b>Sum:</b> The sum of all weights is approximately 260 million, representing the estimated U.S. adult population in 2023.</li>\n",
    "        <li><b>Median:</b> A typical respondent represents roughly 14,600 people.</li>\n",
    "        <li><b>Right-Skewed Distribution:</b> The mean (17,584) is higher than the median, confirming that a small number of respondents represent a disproportionately large share of the population.</li>\n",
    "        <li><b>Sampling Strategy:</b> Weights range from 502 to 131,657. This reflects MEPS's strategy of oversampling specific subgroups to ensure reliable estimates for minority or high-need populations.</li>\n",
    "        <li><b>Bias Correction:</b> Because weights vary significantly (std ‚âà 12,334), unweighted models or averages would be biased. Using <code>sample_weight</code> during training ensures the model's loss function prioritizes population representativeness.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ccbccf-4f44-4781-bbd3-702244b2b93a",
   "metadata": {},
   "source": [
    "<a id=\"target-variable\" name=\"target-variable\"></a>\n",
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Target Variable</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Examine descriptive statistics and visualize the distribution of total out-of-pocket health care costs.\n",
    "    <br>\n",
    "    üìù <b>Note:</b> Unless otherwise specified, all descriptive statistics and insights in this section refer to population-level estimates (calculated using sample weights) to ensure representativeness of the U.S. adult population.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc6827-8945-406c-87e1-9c87b9273d40",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Descriptive Statistics</strong> <br>\n",
    "    üìå Examine descriptive statistics of total out-of-pocket health care costs, both on sample-level and population-level. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce0b48d-11c1-4e0a-ba37-43aa93f08548",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Helper function to calculate weighted quantiles (using numpy; not available in pandas)\n",
    "def weighted_quantile(variable, weights, quantile):\n",
    "    sorter = np.argsort(variable)\n",
    "    values = variable.iloc[sorter]\n",
    "    weights = weights.iloc[sorter]\n",
    "    cumulative_weight = np.cumsum(weights) - 0.5 * weights\n",
    "    cumulative_weight /= np.sum(weights)\n",
    "    return np.interp(quantile, cumulative_weight, values)\n",
    "\n",
    "# Helper function to calculate weighted standard deviation (using numpy; not available in pandas)\n",
    "def weighted_std(variable, weights):\n",
    "    weighted_mean = np.average(variable, weights=weights)\n",
    "    weighted_variance = np.average((variable - weighted_mean)**2, weights=weights)\n",
    "    return np.sqrt(weighted_variance)\n",
    "\n",
    "# Calculate weighted (population) statistics\n",
    "pop_mean = np.average(df[\"TOTSLF23\"], weights=df[\"PERWT23F\"])\n",
    "pop_std = weighted_std(df[\"TOTSLF23\"], weights=df[\"PERWT23F\"])\n",
    "pop_p25 = weighted_quantile(df[\"TOTSLF23\"], df[\"PERWT23F\"], 0.25)\n",
    "pop_median = weighted_quantile(df[\"TOTSLF23\"], df[\"PERWT23F\"], 0.5)\n",
    "pop_p75 = weighted_quantile(df[\"TOTSLF23\"], df[\"PERWT23F\"], 0.75)\n",
    "pop_p95 = weighted_quantile(df[\"TOTSLF23\"], df[\"PERWT23F\"], 0.95)\n",
    "pop_p99 = weighted_quantile(df[\"TOTSLF23\"], df[\"PERWT23F\"], 0.99)\n",
    "pop_p999 = weighted_quantile(df[\"TOTSLF23\"], df[\"PERWT23F\"], 0.999)\n",
    "\n",
    "# Calculate sample (unweighted) quantiles\n",
    "sample_p95 = df[\"TOTSLF23\"].quantile(0.95)\n",
    "sample_p99 = df[\"TOTSLF23\"].quantile(0.99)\n",
    "sample_p999 = df[\"TOTSLF23\"].quantile(0.999)\n",
    "\n",
    "# Calculate total costs (sum)\n",
    "sample_total_costs = df[\"TOTSLF23\"].sum()\n",
    "df[\"pop_costs\"] = df[\"TOTSLF23\"] * df[\"PERWT23F\"]  \n",
    "pop_total_costs = df[\"pop_costs\"].sum()\n",
    "\n",
    "# Create comparison table: Sample vs. population statistics \n",
    "sample_vs_population_stats = pd.DataFrame({\n",
    "    \"Sample (Unweighted)\": [\n",
    "        len(df),\n",
    "        df[\"TOTSLF23\"].mean(),\n",
    "        df[\"TOTSLF23\"].std(),\n",
    "        df[\"TOTSLF23\"].min(),\n",
    "        df[\"TOTSLF23\"].quantile(0.25),\n",
    "        df[\"TOTSLF23\"].median(),\n",
    "        df[\"TOTSLF23\"].quantile(0.75),\n",
    "        sample_p95,\n",
    "        sample_p99,\n",
    "        sample_p999,\n",
    "        df[\"TOTSLF23\"].max(),\n",
    "        sample_total_costs\n",
    "    ],\n",
    "    \"Population (Weighted)\": [\n",
    "        df[\"PERWT23F\"].sum(),  # sum of weights = count of target population\n",
    "        pop_mean,\n",
    "        pop_std,\n",
    "        df[\"TOTSLF23\"].min(),  # min is identical\n",
    "        pop_p25,\n",
    "        pop_median,\n",
    "        pop_p75,\n",
    "        pop_p95,\n",
    "        pop_p99,\n",
    "        pop_p999,\n",
    "        df[\"TOTSLF23\"].max(),  # max is identical\n",
    "        pop_total_costs\n",
    "    ]\n",
    "}, index=[\"count\", \"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"95%\", \"99%\", \"99.9%\", \"max\", \"sum\"])\n",
    "\n",
    "# Display table\n",
    "# Formatting: Comma thousand separator and rounded to zero decimals (sample sum in Millions, population sum in Billions with one decimal)\n",
    "sample_vs_population_stats.style.format(\"{:,.0f}\") \\\n",
    "    .format(lambda x: f\"${x/1e6:.1f}M\", subset=(\"sum\", \"Sample (Unweighted)\")) \\\n",
    "    .format(lambda x: f\"${x/1e9:.1f}B\", subset=(\"sum\", \"Population (Weighted)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e499a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f7fff8; padding:15px; border:3px solid #e0f0e0; border-radius:6px;\">\n",
    "    üí° <b>Insight:</b> The descriptive statistics reveal a highly skewed and volatile cost distribution, emphasizing the importance of using sample weights for representative estimates.\n",
    "    <ul style=\"margin-top:10px; margin-bottom:0px\">\n",
    "        <li><b>Right Skewness:</b> The population mean (\\$1,106) is over 4x higher than the median (\\$251), indicating that a few high-cost cases disproportionately influence the average.</li>\n",
    "        <li><b>Sampling Bias Correction:</b> The mean, median and most percentiles (up to the 99th) are lower in the population compared to the sample (e.g., mean drops from \\$1,160 to \\$1,106), showing that the raw sample slightly over-represented most higher-cost individuals. However, the 99.9th percentile and standard deviation are actually higher in the population, revealing that the raw sample slightly under-represented the most extreme tail risk of the \"super-spenders\" (top 0.1%).</li>\n",
    "        <li><b>Extreme Financial Risk:</b> While 75% of the population spends less than \\$1,042 out-of-pocket, the maximum reaches \\$104,652, highlighting severe financial exposure for a minority.</li>\n",
    "        <li><b>High Dispersion:</b> The standard deviation (~\\$3,000) is nearly triple the mean, reflecting the inherent unpredictability and high variance in health care costs.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e54d1bc-fca3-4581-bdb5-767b1c70117d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Histogram</strong> <br> \n",
    "    üìå Visualize the distribution of out-of-pocket health care costs. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e16737-bf84-4a7d-96b8-e8a3f13ed064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram: Sample vs. Population (overlayed)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Population histogram \n",
    "sns.histplot(\n",
    "    data=df, x=\"TOTSLF23\", weights=\"PERWT23F\", label=\"Population (Weighted)\",\n",
    "    stat=\"probability\", bins=50, color=\"#4e8ac8\", alpha=0.5, element=\"bars\"\n",
    ")\n",
    "\n",
    "# Sample histogram \n",
    "sns.histplot(\n",
    "    data=df, x=\"TOTSLF23\", label=\"Sample (Unweighted)\",\n",
    "    stat=\"probability\", bins=50, color=\"#2c699d\", alpha=0.8, element=\"step\"\n",
    ")\n",
    "\n",
    "# Formatting\n",
    "plt.title(\"Distribution of Out-of-Pocket Costs\")\n",
    "plt.xlabel(\"Out-of-Pocket Costs\")\n",
    "plt.ylabel(\"Share\")\n",
    "\n",
    "# Add population mean and median lines for context\n",
    "plt.axvline(pop_mean, color=\"#e63946\", linestyle=\"--\", alpha=0.8, label=f\"Population Mean: ${pop_mean:,.0f}\")\n",
    "plt.axvline(pop_median, color=\"#fb8500\", linestyle=\"--\", alpha=0.8, label=f\"Population Median: ${pop_median:,.0f}\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# Format X-axis as dollars with comma thousand separator rounded to zero decimals\n",
    "plt.gca().xaxis.set_major_formatter(mtick.StrMethodFormatter(\"${x:,.0f}\"))\n",
    "# Format Y-axis as percentages\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c86f99",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Note: Due to the zero-inflation (22% have \\$0 costs) and the extremely heavy tail (max \\$104k), the full distribution is heavily compressed into the first few bins.<br>\n",
    "    üìå Visualize the \"typical\" distribution excluding zero costs and the top 5% of spenders (zooming in).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c337cdd-5394-4406-b298-0ab692ec9dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of typical range (excluding zero costs and top 5%)\n",
    "plot_data = df[(df[\"TOTSLF23\"] > 0) & (df[\"TOTSLF23\"] < pop_p95)].copy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Population histogram \n",
    "sns.histplot(\n",
    "    data=plot_data, x=\"TOTSLF23\", weights=\"PERWT23F\", label=\"Population (Weighted)\",\n",
    "    stat=\"probability\", bins=50, color=\"#4e8ac8\", alpha=0.5, element=\"bars\"\n",
    ")\n",
    "\n",
    "# Sample histogram\n",
    "sns.histplot(\n",
    "    data=plot_data, x=\"TOTSLF23\", label=\"Sample (Unweighted)\",\n",
    "    stat=\"probability\", bins=50, color=\"#2c699d\", alpha=0.8, element=\"step\"\n",
    ")\n",
    "\n",
    "# Add population mean and median lines for context\n",
    "plt.axvline(pop_mean, color=\"#e63946\", linestyle=\"--\", alpha=0.8, label=f\"Population Mean: ${pop_mean:,.0f}\")\n",
    "plt.axvline(pop_median, color=\"#fb8500\", linestyle=\"--\", alpha=0.8, label=f\"Population Median: ${pop_median:,.0f}\")\n",
    "\n",
    "# Formatting\n",
    "plt.title(\"Distribution of Typical Out-of-Pocket Costs (excluding zero costs and top 5%)\")\n",
    "plt.xlabel(\"Out-of-Pocket Costs\")\n",
    "plt.ylabel(\"Share\")\n",
    "plt.legend()\n",
    "\n",
    "# Format X-axis as dollars with thousand separator \n",
    "plt.gca().xaxis.set_major_formatter(mtick.StrMethodFormatter(\"${x:,.0f}\"))\n",
    "# Format Y-axis as percentages\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d917f202",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f7fff8; padding:15px; border:3px solid #e0f0e0; border-radius:6px;\">\n",
    "    üí° <b>Insight:</b> The \"typical\" distribution (excluding zeros and the top 5%) remains heavily right-skewed with a massive share of the population still concentrated in the lowest cost bins. To get a better picture, I will conduct in-depth Zero Costs Analysis, Lorenz Curve, Cost Concentration Analysis, and Top 1% Analysis.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359a197c-f752-447f-a8e2-83a4291bb667",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Zero Costs Analysis</strong> <br>\n",
    "    üìå Deeper analysis of people with zero out-of-pocket health care costs (sample and population). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9427d8e-502b-4502-9f54-640ea64f1290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero costs analysis\n",
    "zero_costs_df = pd.DataFrame({\n",
    "    \"Sample (Unweighted) Count\": [len(df[df[\"TOTSLF23\"] == 0]), len(df[df[\"TOTSLF23\"] > 0])],\n",
    "    \"Population (Weighted) Count\": [df[df[\"TOTSLF23\"] == 0][\"PERWT23F\"].sum(), df[df[\"TOTSLF23\"] > 0][\"PERWT23F\"].sum()],\n",
    "    \"Sample (Unweighted) %\": [\n",
    "        (df[\"TOTSLF23\"] == 0).mean() * 100,\n",
    "        (df[\"TOTSLF23\"] > 0).mean() * 100\n",
    "    ],\n",
    "    \"Population (Weighted) %\": [\n",
    "        (df.loc[df[\"TOTSLF23\"] == 0, \"PERWT23F\"].sum() / df[\"PERWT23F\"].sum()) * 100,\n",
    "        (df.loc[df[\"TOTSLF23\"] > 0, \"PERWT23F\"].sum() / df[\"PERWT23F\"].sum()) * 100\n",
    "    ]\n",
    "}, index=[\"Zero Costs\", \"Positive Costs\"]).round(2)\n",
    "\n",
    "zero_costs_df.style.format({\n",
    "    \"Sample (Unweighted) Count\": \"{:,.0f}\",\n",
    "    \"Population (Weighted) Count\": \"{:,.0f}\",\n",
    "    \"Sample (Unweighted) %\": \"{:.1f}%\",\n",
    "    \"Population (Weighted) %\": \"{:.1f}%\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1729385a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f7fff8; padding:15px; border:3px solid #e0f0e0; border-radius:6px;\">\n",
    "    üí° <b>Insight:</b> The large proportion of zeros confirms that the target variable is zero-inflated.\n",
    "    <ul style=\"margin-top:10px; margin-bottom:0px\">\n",
    "        <li><b>High Zero-Cost Prevalence:</b> Over 22% of the U.S. adult population (approx. 58 million people) had zero out-of-pocket health care costs in 2023.</li>\n",
    "        <li><b>Correction of Sampling Bias:</b> The weighted population percentage (22.3%) is higher than the unweighted sample percentage (20.7%), indicating that zero-cost individuals were slightly under-represented in the raw survey data.</li>\n",
    "        <li><b>Modeling Implications:</b> The zero-inflated target variable suggests that a two-part modeling strategy (e.g., predicting the probability of any spend vs. the amount of spend) may be more effective than a single standard regression.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6ca97-604a-4728-8d01-2a0d607a2115",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Lorenz Curve</strong> <br>\n",
    "    üìå Plot the Lorenz Curve. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f21edb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lorenz Curve\n",
    "# Create DataFrame for plotting the Lorenz Curve sorted by cost \n",
    "lorenz_df = df[[\"TOTSLF23\", \"PERWT23F\", \"pop_costs\"]].sort_values(\"TOTSLF23\").copy()\n",
    "\n",
    "# Cumulative percentage and costs of sample (unweighted) \n",
    "cum_sample_pct = np.arange(1, len(lorenz_df) + 1) / len(lorenz_df) * 100\n",
    "cum_sample_costs = lorenz_df[\"TOTSLF23\"].cumsum() / lorenz_df[\"TOTSLF23\"].sum() * 100\n",
    "\n",
    "# Cumulative percentage and costs of population (weighted)\n",
    "cum_pop_pct = lorenz_df[\"PERWT23F\"].cumsum() / lorenz_df[\"PERWT23F\"].sum() * 100\n",
    "cum_pop_costs = lorenz_df[\"pop_costs\"].cumsum() / lorenz_df[\"pop_costs\"].sum() * 100\n",
    "\n",
    "# Calculate the Gini Coefficient \n",
    "# Note: Quantifies inequality (the higher the number, the more the cost is concentrated) \n",
    "# Gini = A / (A + B), which simplifies to 1 - 2*B, where B is the area under the Lorenz Curve (calculated via the trapezoidal rule)\n",
    "def calculate_gini(pct, costs):\n",
    "    return 1 - 2 * np.trapezoid(costs / 100, pct / 100)\n",
    "\n",
    "gini_sample = calculate_gini(cum_sample_pct, cum_sample_costs)\n",
    "gini_pop = calculate_gini(cum_pop_pct, cum_pop_costs)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(9, 7))  \n",
    "\n",
    "# Line of Equality\n",
    "plt.plot([0, 100], [0, 100], linestyle=\"--\", color=\"gray\", label=\"Line of Equality\", alpha=0.6)\n",
    "\n",
    "# Population Lorenz Curve\n",
    "plt.plot(cum_pop_pct, cum_pop_costs, \n",
    "         label=f\"Population (Gini: {gini_pop:.2f})\", \n",
    "         color=\"#084594\", lw=2.5)\n",
    "\n",
    "# Sample Lorenz Curve\n",
    "plt.plot(cum_sample_pct, cum_sample_costs, \n",
    "         label=f\"Sample (Gini: {gini_sample:.2f})\", \n",
    "         color=\"#4e8ac8\", lw=1.5, linestyle=\":\")\n",
    "\n",
    "# Highlight Pareto Point (80/20 Rule) \n",
    "idx_80 = (cum_pop_pct - 80).abs().idxmin()  # find the index label closest to the 80th percentile of the population\n",
    "x_80 = cum_pop_pct.loc[idx_80]\n",
    "y_80 = cum_pop_costs.loc[idx_80]\n",
    "top_20_share = 100 - y_80\n",
    "\n",
    "plt.plot(x_80, y_80, 'o', color=\"#fb8500\", markersize=8)\n",
    "plt.annotate(f\"Top 20% account for\\n{top_20_share:.1f}% of costs\", \n",
    "             xy=(x_80, y_80), xytext=(x_80 - 30, y_80 + 10),\n",
    "             arrowprops=dict(arrowstyle=\"->\", color=\"black\", connectionstyle=\"arc3,rad=-0.2\", alpha=0.8),\n",
    "             fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "# Highlight Zero-Cost Threshold (the point where costs begin to rise)\n",
    "pop_zero_pct = (df.loc[df[\"TOTSLF23\"] == 0, \"PERWT23F\"].sum() / df[\"PERWT23F\"].sum()) * 100\n",
    "plt.plot(pop_zero_pct, 0, 'o', color=\"#fb8500\", markersize=8)\n",
    "plt.annotate(f\"{pop_zero_pct:.1f}% have $0 costs\", \n",
    "             xy=(pop_zero_pct, 0), xytext=(pop_zero_pct - 7, 10),\n",
    "             arrowprops=dict(arrowstyle=\"->\", color=\"black\", connectionstyle=\"arc3,rad=-0.2\", alpha=0.8),\n",
    "             fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "# Fill for emphasis\n",
    "plt.fill_between(cum_pop_pct, cum_pop_costs, cum_pop_pct, color=\"#084594\", alpha=0.05)\n",
    "\n",
    "# Customization\n",
    "plt.title(\"Lorenz Curve: Concentration of Out-of-Pocket Costs\", fontsize=14, pad=15)\n",
    "plt.xlabel(\"Cumulative % of U.S. Adults (Sorted from Lowest to Highest Cost)\", fontsize=11)\n",
    "plt.ylabel(\"Cumulative % of Total Costs\", fontsize=11)\n",
    "plt.legend(loc=\"upper left\", fontsize=10)\n",
    "plt.grid(True, alpha=0.2)\n",
    "plt.xticks(range(0, 101, 10))\n",
    "plt.yticks(range(0, 101, 10))\n",
    "plt.gca().xaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a087fc-cbca-4224-a3a9-f3a86f8a266a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f7fff8; padding:15px; border:3px solid #e0f0e0; border-radius:6px;\">\n",
    "    üí° <b>Insight:</b> The Lorenz Curve reveals extreme inequality in out-of-pocket health care spending, far exceeding typical measures of economic inequality.\n",
    "    <ul style=\"margin-top:10px; margin-bottom:0px\">\n",
    "        <li><b>The 80/20 Rule:</b> The top 20% of spenders account for <b>79.3%</b> of total costs, almost perfectly reflecting the Pareto Principle.</li>\n",
    "        <li><b>Inequality Comparison:</b> A Gini coefficient of <b>0.77</b> represents massive concentration. For context, U.S. income inequality (Gini ~0.45) is often considered high; healthcare cost inequality is significantly more than wealth.</li>\n",
    "        <li><b>The \"Low-Cost\" Majority:</b> The curve remains flat for the first <b>70%</b> of the population, who combined account for only about <b>12%</b> of total out-of-pocket costs.</li>\n",
    "        <li><b>Zero Costs:</b> A significant \"hurdle\" for modeling is the <b>22.3%</b> of U.S. adults who incur $0 in costs, requiring a model capable of handling zero-inflation.</li>\n",
    "    </ul>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3aa8ba",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Cost Concentration Analysis</strong> <br>\n",
    "    üìå Examine the cost thresholds, totals, and shares for the top 1%, 5%, 10%, 20%, and 50% of spenders (sample and population). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7bd29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost Concentration Benchmarks \n",
    "percentiles = [0.99, 0.95, 0.9, 0.8, 0.5]\n",
    "stats = []\n",
    "\n",
    "sample_total_costs = df[\"TOTSLF23\"].sum()\n",
    "pop_total_costs = df[\"pop_costs\"].sum()\n",
    "\n",
    "for p in percentiles:\n",
    "    # Calculate Thresholds\n",
    "    sample_threshold = df[\"TOTSLF23\"].quantile(p)\n",
    "    pop_threshold = weighted_quantile(df[\"TOTSLF23\"], df[\"PERWT23F\"], p)\n",
    "    \n",
    "    # Sample Share (Unweighted)\n",
    "    sample_share = (df[df[\"TOTSLF23\"] >= sample_threshold][\"TOTSLF23\"].sum() / sample_total_costs) * 100\n",
    "    \n",
    "    # Population Share (Weighted)\n",
    "    pop_share = (df[df[\"TOTSLF23\"] >= pop_threshold][\"pop_costs\"].sum() / pop_total_costs) * 100\n",
    "    \n",
    "    stats.append({\n",
    "        \"Top X%\": f\"Top {(1-p)*100:.0f}%\",\n",
    "        \"Threshold (Sample)\": sample_threshold,\n",
    "        \"Threshold (Population)\": pop_threshold,\n",
    "        \"Share of Total Costs (Sample)\": sample_share,\n",
    "        \"Share of Total Costs (Population)\": pop_share\n",
    "    })\n",
    "\n",
    "# Set \"Top X%\" as DataFrame index\n",
    "cost_concentration_df = pd.DataFrame(stats).set_index(\"Top X%\")\n",
    "\n",
    "# Show table with formatted values\n",
    "cost_concentration_df.style.format({\n",
    "    \"Threshold (Sample)\": \"${:,.0f}\",\n",
    "    \"Threshold (Population)\": \"${:,.0f}\",\n",
    "    \"Share of Total Costs (Sample)\": \"{:.1f}%\",\n",
    "    \"Share of Total Costs (Population)\": \"{:.1f}%\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e3098b-cec3-47e8-8f03-dd400ce032f4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f7fff8; padding:15px; border:3px solid #e0f0e0; border-radius:6px;\">\n",
    "    üí° <b>Insights:</b> The cost concentration analysis quantifies the \"heavy tail\" of the distribution:\n",
    "    <ul style=\"margin-top:10px; margin-bottom:0px\">\n",
    "        <li><b>Top 1% vs. Bottom 50%:</b> The top 1% of spenders account for <b>20.6%</b> of total costs. This is nearly <b>ten times</b> more than the bottom 50% of the population combined (2.4%).</li>\n",
    "        <li><b>Exponential Escalation:</b> The cost threshold more than <b>doubles</b> between the top 5% ($4,518) and the top 1% (\\$12,868), indicating that financial risk increases exponentially as one moves into the tail.</li>\n",
    "        <li><b>Predictive Challenge:</b> Because the bottom half of the population contributes so little to the total sum, a model that focuses on the \"average\" person will be highly accurate for the majority but fail catastrophically for the \"super-spenders\" who drive the actual financial risk.</li>\n",
    "    </ul>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c8d3bb-dc36-47d7-b95c-8358b37a2ec4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Top 1% Analysis</strong> <br>\n",
    "    üìå Deeper analysis of respondents in the top 1% and top 0.1% of out-of-pocket health care costs to understand extreme tail risk. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b218849-3849-4c3d-9f24-cc07d5615a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 1% analysis\n",
    "# Sample (Unweighted)\n",
    "sample_top_1_costs = df[df[\"TOTSLF23\"] >= sample_p99][\"TOTSLF23\"].sum()\n",
    "sample_top_01_costs = df[df[\"TOTSLF23\"] >= sample_p999][\"TOTSLF23\"].sum()\n",
    "\n",
    "# Population (Weighted)\n",
    "pop_top_1_costs = df[df[\"TOTSLF23\"] >= pop_p99][\"pop_costs\"].sum()\n",
    "pop_top_01_costs = df[df[\"TOTSLF23\"] >= pop_p999][\"pop_costs\"].sum()\n",
    "\n",
    "# Create comparison table\n",
    "top_1_df = pd.DataFrame({\n",
    "    \"Sample (Unweighted)\": [\n",
    "        sample_p99,\n",
    "        sample_top_1_costs / 1e6,  # Millions\n",
    "        (sample_top_1_costs / sample_total_costs) * 100,\n",
    "        sample_p999,\n",
    "        sample_top_01_costs / 1e6,  # Millions\n",
    "        (sample_top_01_costs / sample_total_costs) * 100\n",
    "    ],\n",
    "    \"Population (Weighted)\": [\n",
    "        pop_p99,\n",
    "        pop_top_1_costs / 1e9,  # Billions\n",
    "        (pop_top_1_costs / pop_total_costs) * 100,\n",
    "        pop_p999,\n",
    "        pop_top_01_costs / 1e9,  # Billions\n",
    "        (pop_top_01_costs / pop_total_costs) * 100\n",
    "    ]\n",
    "}, index=[\n",
    "    \"Top 1% Threshold\",\n",
    "    \"Top 1% Total Costs\",\n",
    "    \"Top 1% Share of Costs\",\n",
    "    \"Top 0.1% Threshold\",\n",
    "    \"Top 0.1% Total Costs\",\n",
    "    \"Top 0.1% Share of Costs\"\n",
    "])\n",
    "\n",
    "# Style the table\n",
    "top_1_df.style.format(\"${:,.0f}\", subset=([\"Top 0.1% Threshold\", \"Top 1% Threshold\"], slice(None))) \\\n",
    "                 .format(\"${:,.1f}M\", subset=([\"Top 0.1% Total Costs\", \"Top 1% Total Costs\"], \"Sample (Unweighted)\")) \\\n",
    "                 .format(\"${:,.1f}B\", subset=([\"Top 0.1% Total Costs\", \"Top 1% Total Costs\"], \"Population (Weighted)\")) \\\n",
    "                 .format(\"{:.1f}%\", subset=([\"Top 0.1% Share of Costs\", \"Top 1% Share of Costs\"], slice(None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aa0c7a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f7fff8; padding:15px; border:3px solid #e0f0e0; border-radius:6px;\">\n",
    "    üí° <b>Insight:</b> Extreme cost concentration in the tail of the distribution.\n",
    "    <ul style=\"margin-top:10px; margin-bottom:0px\">\n",
    "        <li><b>The 1% Rule:</b> The top 1% of spenders ‚Äîthose spending over ~\\$13k‚Äîaccount for roughly 20% of all out-of-pocket costs.</li>\n",
    "        <li><b>The Hyper-Tail (Top 0.1%):</b> The top 0.1% of spenders‚Äîthose spending over ~\\$39k‚Äîaccount for a disproportionate share of total costs (approx. 5%). This highlights that the tail is not just long, but extremely heavy.</li>\n",
    "        <li><b>Extreme Outliers:</b> The gap between the 99th percentile (\\$12,868) and the maximum (\\$104,652) is massive. These \"super-spenders\" represent a significant challenge for predictive modeling, as a single misprediction here could lead to very high error (RMSE).</li>\n",
    "        <li><b>Financial Risk Benchmarks:</b> These thresholds provide a clear picture of what \"catastrophic\" spending looks like in the U.S. adult population.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc51cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the \"Super-Spenders\" (Everyone in the Top 0.1% of the population)\n",
    "# This helps us identify if there are common patterns (e.g., age or chronic conditions) among extreme spenders.\n",
    "chronic_cols = [\"HIBPDX\", \"CHOLDX\", \"DIABDX_M18\", \"CHDDX\", \"STRKDX\", \"CANCERDX\", \"ARTHDX\", \"ASTHDX\"]\n",
    "df[\"CHRONIC_COUNT\"] = (df[chronic_cols] == 1).sum(axis=1)\n",
    "\n",
    "super_spenders = df[df[\"TOTSLF23\"] >= pop_p999].sort_values(\"TOTSLF23\", ascending=False)\n",
    "super_spenders[[\"TOTSLF23\", \"PERWT23F\", \"AGE23X\", \"SEX\", \"INSCOV23\", \"CHRONIC_COUNT\"] + chronic_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70d5f2a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f7fff8; padding:15px; border:3px solid #e0f0e0; border-radius:6px;\">\n",
    "    üí° <b>Insight:</b> The \"Super-Spender\" profiles reveal a critical dichotomy for predictive modeling.\n",
    "    <ul style=\"margin-top:10px; margin-bottom:0px\">\n",
    "        <li><b>Acute vs. Chronic Drivers:</b> We see two distinct profiles: \"Multi-Morbid\" elderly (e.g., 85yo with 7 conditions) and \"Acute Shock\" individuals (young with 0-1 conditions). This suggests that while chronic disease predicts higher costs on average, the extreme tail is often driven by unpredictable acute events (accidents, rare surgeries).</li>\n",
    "        <li><b>The \"Black Swan\":</b> The absolute maximum outlier is a 24-year-old with only 1 chronic condition but ~\\$104k in out-of-pocket costs. With a high sample weight (~31k), this single respondent represents over 30,000 people. This is a \"Black Swan\" event‚Äîatypical and unpredictable‚Äîthat could severely impair model performance if not handled carefully. Mathematically, because RMSE squares the error, a single \\$50,000 prediction error on the top outlier adds as much to the loss function as \\$1,500 errors on 1,000 different people. The model will be naturally \"obsessed\" with fitting these few outliers at the expense of the general population.</li>\n",
    "        <li><b>The Insurance Paradox:</b> Surprisingly, only one of the top 15 spenders is uninsured (<code>INSCOV23=3</code>). Most even have private insurance (n=11). This shows that high out-of-pocket costs in the U.S. are not just a problem for the uninsured, but often hit those with coverage who face high deductibles, co-pays, or non-covered services.</li>\n",
    "        <li><b>Strategy:</b> To prevent these outliers from dominating the training, we must consider target transformations (Log/Box-Cox) or robust regression techniques that \"downweight\" the influence of extreme residuals.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b590b9e3-8d07-47d2-a81b-068c1fbd2843",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4e8ac8; color:white; padding:10px; border-radius:6px;\">\n",
    "    <h3 style=\"margin:0px\">Numerical Features</h3>\n",
    "</div> \n",
    "\n",
    "<div style=\"background-color:#e8f4fd; padding:15px; border:3px solid #d0e7fa; border-radius:6px;\">\n",
    "    ‚ÑπÔ∏è Examine descriptive statistics and visualize the distributions of numerical features.\n",
    "    <br><br>\n",
    "    ‚ö†Ô∏è Note: Conduct this EDA part exclusively on the <b>training data</b> to prevent data leakage, as these statistics will directly inform the data preprocessing strategy (e.g., handling missing values). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec93bb2-aca0-4aa5-bfae-22c2afacf6e4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Descriptive Statistics</strong> <br>\n",
    "    üìå Examine descriptive statistics (e.g., mean, median, standard deviation) of all numerical features.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460cb60-e0de-4db5-988d-5761dc4e8525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics of numerical features\n",
    "X_train[numerical_features].describe().T.style.format({\n",
    "    \"count\": \"{:,.0f}\",\n",
    "    \"mean\": \"{:.2f}\",\n",
    "    \"std\": \"{:.2f}\",\n",
    "    \"min\": \"{:.1f}\",\n",
    "    \"25%\": \"{:.1f}\",\n",
    "    \"50%\": \"{:.1f}\",\n",
    "    \"75%\": \"{:.1f}\",\n",
    "    \"max\": \"{:.1f}\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c2d8f1-8dd3-4242-a752-496d8d1d57eb",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f7fff8; padding:15px; border:3px solid #e0f0e0; border-radius:6px;\">\n",
    "    üí° <b>Insight:</b> Descriptive statistics justify the use of median imputation for numerical features.\n",
    "    <ul style=\"margin-top:10px; margin-bottom:0px\">\n",
    "        <li><b>Robustness to Skewness:</b> For <code>FAMSZE23</code>, the mean (2.69) is notably higher than the median (2.0) due to extreme outliers (max 14). Imputing the median prevents these \"high-cost\" family sizes from biasing our guess for typical respondents.</li>\n",
    "        <li><b>Maintaining Scale Integrity:</b> Health status variables (<code>RTHLTH31</code>, <code>MNHLTH31</code>) use a discrete 1‚Äì5 scale. Median imputation ensures that missing values are filled with actual observed integers (e.g., 2.0) rather than non-existent decimals (e.g., 2.46).</li>\n",
    "        <li><b>Minimal Imputation Bias:</b> Given the very low missingness (maximum ~0.3%), the choice of median vs. mean will not significantly impact model performance, but median remains the statistically sounder choice.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4592f8-ed88-4f01-a49b-450ec325fa51",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6e4; padding:15px; border:3px solid #f5ecda; border-radius:6px;\">\n",
    "    <strong>Visualize Distributions</strong> <br> \n",
    "    üìå Plot a histogram matrix that shows the distributions of all numerical features. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f52f26-a956-41eb-a32f-989328955535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2x2 subplot grid\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "# Flatten the axes for easier iteration\n",
    "axes = axes.flat\n",
    "\n",
    "# Iterate over all numerical features\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    # Get the current axes\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Create histogram for the current feature\n",
    "    sns.histplot(\n",
    "        data=X_train, \n",
    "        x=feature, \n",
    "        ax=ax,\n",
    "        discrete=True,                               # Centers bars on integers \n",
    "        kde=True if feature == \"AGE23X\" else False,  # Adds a density curve for age\n",
    "        edgecolor=\"white\",                           # Adds white border lines between bars\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "    # Customize histogram\n",
    "    ax.set_title(feature, fontsize=14, fontweight=\"bold\") \n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Frequency\" if i % 2 == 0 else \"\", fontsize=12) # Only y-label on left plots\n",
    "    ax.grid(True, axis=\"y\", alpha=0.3)  # Adds grid lines\n",
    "    sns.despine(ax=ax)  # Removes top & right spines\n",
    "\n",
    "fig.tight_layout()  # Adjusts layout to prevent overlap\n",
    "\n",
    "# Show histogram matrix\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Medical Cost Prediction",
   "language": "python",
   "name": "medical_cost_prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
